
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/01-light_curves/neural_network_brightness.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_01-light_curves_neural_network_brightness.py:


Neural Network Brightness
=========================

Trains a neural network to predict the brightness of a specular cube in an arbitrary lighting and observation conditions and compares the results to the truth

.. GENERATED FROM PYTHON SOURCE LINES 7-15

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    import seaborn as sns

    import mirage as mr
    import mirage.sim as mrsim








.. GENERATED FROM PYTHON SOURCE LINES 16-17

Let's define the object and the BRDF

.. GENERATED FROM PYTHON SOURCE LINES 17-19

.. code-block:: Python

    obj = mr.SpaceObject('cube.obj')
    brdf = mr.Brdf('phong', cd=0.5, cs=0.5, n=10)







.. GENERATED FROM PYTHON SOURCE LINES 20-21

We now define the Multi-Layer Perceptron (MLP) brightness model. Note that the ``layers=(150, 50, 150)`` keyword argument defines the number of neurons in each densely-connected layer.

.. GENERATED FROM PYTHON SOURCE LINES 21-22

.. code-block:: Python

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=False)







.. GENERATED FROM PYTHON SOURCE LINES 23-24

Now we train the model on a set number of training lighting and observation configurations. Usually ``1e5``-``1e6`` are required for a *good* fit

.. GENERATED FROM PYTHON SOURCE LINES 24-27

.. code-block:: Python

    num_train = int(1e3)
    mlp_bm.train(num_train)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 3.57e-03 seconds
    Iteration 1, loss = 0.24003164
    Iteration 2, loss = 0.17039077
    Iteration 3, loss = 0.15032812
    Iteration 4, loss = 0.14594405
    Iteration 5, loss = 0.14292076
    Iteration 6, loss = 0.14338061
    Iteration 7, loss = 0.14001909
    Iteration 8, loss = 0.13964403
    Iteration 9, loss = 0.13741189
    Iteration 10, loss = 0.13792465
    Iteration 11, loss = 0.13606309
    Iteration 12, loss = 0.13509252
    Iteration 13, loss = 0.13228966
    Iteration 14, loss = 0.13058459
    Iteration 15, loss = 0.12871338
    Iteration 16, loss = 0.12683759
    Iteration 17, loss = 0.12500845
    Iteration 18, loss = 0.12246749
    Iteration 19, loss = 0.11959367
    Iteration 20, loss = 0.11881921
    Iteration 21, loss = 0.11437952
    Iteration 22, loss = 0.11281507
    Iteration 23, loss = 0.10809076
    Iteration 24, loss = 0.10467901
    Iteration 25, loss = 0.09957760
    Iteration 26, loss = 0.09635492
    Iteration 27, loss = 0.09155116
    Iteration 28, loss = 0.08881381
    Iteration 29, loss = 0.08435692
    Iteration 30, loss = 0.08034116
    Iteration 31, loss = 0.07468218
    Iteration 32, loss = 0.06999340
    Iteration 33, loss = 0.06694065
    Iteration 34, loss = 0.06230975
    Iteration 35, loss = 0.06414580
    Iteration 36, loss = 0.06123574
    Iteration 37, loss = 0.05694324
    Iteration 38, loss = 0.05455433
    Iteration 39, loss = 0.05236931
    Iteration 40, loss = 0.04863142
    Iteration 41, loss = 0.04654846
    Iteration 42, loss = 0.04436595
    Iteration 43, loss = 0.04419751
    Iteration 44, loss = 0.04160665
    Iteration 45, loss = 0.03947190
    Iteration 46, loss = 0.03754199
    Iteration 47, loss = 0.03640177
    Iteration 48, loss = 0.03423447
    Iteration 49, loss = 0.03386362
    Iteration 50, loss = 0.03314928
    Iteration 51, loss = 0.03225923
    Iteration 52, loss = 0.03153148
    Iteration 53, loss = 0.02954587
    Iteration 54, loss = 0.02895922
    Iteration 55, loss = 0.02902374
    Iteration 56, loss = 0.02828418
    Iteration 57, loss = 0.02636530
    Iteration 58, loss = 0.02474848
    Iteration 59, loss = 0.02447757
    Iteration 60, loss = 0.02262275
    Iteration 61, loss = 0.02265658
    Iteration 62, loss = 0.02346527
    Iteration 63, loss = 0.02053479
    Iteration 64, loss = 0.02088483
    Iteration 65, loss = 0.01892209
    Iteration 66, loss = 0.01702803
    Iteration 67, loss = 0.01865028
    Iteration 68, loss = 0.01772481
    Iteration 69, loss = 0.01554127
    Iteration 70, loss = 0.01523932
    Iteration 71, loss = 0.01550567
    Iteration 72, loss = 0.01494005
    Iteration 73, loss = 0.01453267
    Iteration 74, loss = 0.01370030
    Iteration 75, loss = 0.01283128
    Iteration 76, loss = 0.01217649
    Iteration 77, loss = 0.01199273
    Iteration 78, loss = 0.01200786
    Iteration 79, loss = 0.01174397
    Iteration 80, loss = 0.01051106
    Iteration 81, loss = 0.01037634
    Iteration 82, loss = 0.01036734
    Iteration 83, loss = 0.01070931
    Iteration 84, loss = 0.00960716
    Iteration 85, loss = 0.01040221
    Iteration 86, loss = 0.00931768
    Iteration 87, loss = 0.00925078
    Iteration 88, loss = 0.00831055
    Iteration 89, loss = 0.00816500
    Iteration 90, loss = 0.00820699
    Iteration 91, loss = 0.00895653
    Iteration 92, loss = 0.00922112
    Iteration 93, loss = 0.00826305
    Iteration 94, loss = 0.00858382
    Iteration 95, loss = 0.00764658
    Iteration 96, loss = 0.00727342
    Iteration 97, loss = 0.00682246
    Iteration 98, loss = 0.00710070
    Iteration 99, loss = 0.00640863
    Iteration 100, loss = 0.00614529
    Iteration 101, loss = 0.00823919
    Iteration 102, loss = 0.00663374
    Iteration 103, loss = 0.00620445
    Iteration 104, loss = 0.00564726
    Iteration 105, loss = 0.00526283
    Iteration 106, loss = 0.00482779
    Iteration 107, loss = 0.00515821
    Iteration 108, loss = 0.00477637
    Iteration 109, loss = 0.00482150
    Iteration 110, loss = 0.00465646
    Iteration 111, loss = 0.00421658
    Iteration 112, loss = 0.00442623
    Iteration 113, loss = 0.00456111
    Iteration 114, loss = 0.00476886
    Iteration 115, loss = 0.00455624
    Iteration 116, loss = 0.00402132
    Iteration 117, loss = 0.00365024
    Iteration 118, loss = 0.00360502
    Iteration 119, loss = 0.00357781
    Iteration 120, loss = 0.00367062
    Iteration 121, loss = 0.00377271
    Iteration 122, loss = 0.00372144
    Iteration 123, loss = 0.00360706
    Iteration 124, loss = 0.00438154
    Iteration 125, loss = 0.00380817
    Iteration 126, loss = 0.00363965
    Iteration 127, loss = 0.00370947
    Iteration 128, loss = 0.00369519
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 3.08e+00 seconds




.. GENERATED FROM PYTHON SOURCE LINES 28-29

We can now simulate a torque-free attitude profile to inspect the quality of the fit

.. GENERATED FROM PYTHON SOURCE LINES 29-40

.. code-block:: Python

    t_eval = np.linspace(0, 10, 1000)
    q, _ = mr.propagate_attitude_torque_free(
        np.array([0.0, 0.0, 0.0, 1.0]),
        np.array([1.0, 1.0, 1.0]),
        np.diag([1, 2, 3]),
        t_eval,
    )
    dcm = mr.quat_to_dcm(q)
    ovb = mr.stack_mat_mult_vec(dcm, np.array([[1, 0, 0]]))
    svb = mr.stack_mat_mult_vec(dcm, np.array([[0, 1, 0]]))








.. GENERATED FROM PYTHON SOURCE LINES 41-42

Evaluating the model in its two available formats - as a native ``scikit-learn`` model and as an Open Neural Network eXchange (ONNX) model

.. GENERATED FROM PYTHON SOURCE LINES 42-49

.. code-block:: Python

    mr.tic('Evaluate trained model with sklearn')
    mdl_b_sklearn = mlp_bm.eval(ovb, svb, eval_mode_pref='sklearn')
    mr.toc()
    mr.tic('Evaluate trained model with onnx')
    mdl_b_onnx = mlp_bm.eval(ovb, svb, eval_mode_pref='onnx')
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate trained model with sklearn: 1.33e-01 seconds
    Evaluate trained model with onnx: 1.43e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 50-51

We can save both of these representations to file:

.. GENERATED FROM PYTHON SOURCE LINES 51-54

.. code-block:: Python

    mlp_bm.save_to_file(save_as_format='onnx')
    mlp_bm.save_to_file(save_as_format='sklearn')








.. GENERATED FROM PYTHON SOURCE LINES 55-56

Now we load the model from its ``.onxx`` file we just saved and evaluate the brightness

.. GENERATED FROM PYTHON SOURCE LINES 56-61

.. code-block:: Python

    mlp_bm.load_from_file(mlp_bm.onnx_file_name)
    mr.tic('Evaluate loaded model with onxx')
    mdl_onnx_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref='onnx')
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with onxx: 1.21e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 62-63

And we do the same for the ``scikit-learn`` ``.plk`` file we saved

.. GENERATED FROM PYTHON SOURCE LINES 63-68

.. code-block:: Python

    mlp_bm.load_from_file(mlp_bm.sklearn_file_name)
    mr.tic('Evaluate loaded model with sklearn')
    mdl_sklearn_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref='sklearn')
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with sklearn: 1.37e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 69-70

We can easily confirm that all four model evaluations have produced the same prediction

.. GENERATED FROM PYTHON SOURCE LINES 70-75

.. code-block:: Python

    print(np.max(np.abs(mdl_b_sklearn - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_sklearn - mdl_sklearn_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_sklearn_loaded)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    9.803612539549533e-07
    0.0
    0.0
    9.803612539549533e-07




.. GENERATED FROM PYTHON SOURCE LINES 76-77

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 77-89

.. code-block:: Python

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_sklearn, errorbar=None)
    plt.title(f'Light Curves for {obj.file_name}, {num_train} Training Points')
    plt.xlabel('Time [s]')
    plt.ylabel('Normalized brightness')
    plt.legend(['True', 'Model'])
    plt.grid()
    plt.show()




.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001_2_00x.png 2.00x
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 90-91

We can also train on magnitude data instead of irradiance:

.. GENERATED FROM PYTHON SOURCE LINES 91-98

.. code-block:: Python

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=True)
    mlp_bm.train(num_train)

    mr.tic('Evaluate trained model with onnx')
    mdl_b_onnx = mlp_bm.eval(ovb, svb)
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 5.78e-01 seconds
    Iteration 1, loss = 0.19916559
    Iteration 2, loss = 0.15145301
    Iteration 3, loss = 0.13038130
    Iteration 4, loss = 0.12741581
    Iteration 5, loss = 0.12247030
    Iteration 6, loss = 0.12183764
    Iteration 7, loss = 0.11969218
    Iteration 8, loss = 0.11834307
    Iteration 9, loss = 0.11662133
    Iteration 10, loss = 0.11544974
    Iteration 11, loss = 0.11436158
    Iteration 12, loss = 0.11295722
    Iteration 13, loss = 0.11204554
    Iteration 14, loss = 0.11082225
    Iteration 15, loss = 0.10877638
    Iteration 16, loss = 0.10717450
    Iteration 17, loss = 0.10570623
    Iteration 18, loss = 0.10409493
    Iteration 19, loss = 0.10177233
    Iteration 20, loss = 0.10071173
    Iteration 21, loss = 0.09825967
    Iteration 22, loss = 0.09528121
    Iteration 23, loss = 0.09352938
    Iteration 24, loss = 0.09003099
    Iteration 25, loss = 0.08729175
    Iteration 26, loss = 0.08470569
    Iteration 27, loss = 0.08196357
    Iteration 28, loss = 0.07874974
    Iteration 29, loss = 0.07747214
    Iteration 30, loss = 0.07563363
    Iteration 31, loss = 0.07075577
    Iteration 32, loss = 0.06721651
    Iteration 33, loss = 0.06402469
    Iteration 34, loss = 0.06164439
    Iteration 35, loss = 0.05708354
    Iteration 36, loss = 0.05597467
    Iteration 37, loss = 0.05108963
    Iteration 38, loss = 0.04919290
    Iteration 39, loss = 0.04874157
    Iteration 40, loss = 0.04551492
    Iteration 41, loss = 0.04427431
    Iteration 42, loss = 0.04215469
    Iteration 43, loss = 0.04015163
    Iteration 44, loss = 0.03954043
    Iteration 45, loss = 0.03825142
    Iteration 46, loss = 0.03789754
    Iteration 47, loss = 0.03607507
    Iteration 48, loss = 0.03545925
    Iteration 49, loss = 0.03549971
    Iteration 50, loss = 0.03727566
    Iteration 51, loss = 0.03172963
    Iteration 52, loss = 0.03009141
    Iteration 53, loss = 0.02903818
    Iteration 54, loss = 0.02859120
    Iteration 55, loss = 0.02696663
    Iteration 56, loss = 0.02608813
    Iteration 57, loss = 0.02441638
    Iteration 58, loss = 0.02316920
    Iteration 59, loss = 0.02244456
    Iteration 60, loss = 0.02324489
    Iteration 61, loss = 0.02582142
    Iteration 62, loss = 0.02335841
    Iteration 63, loss = 0.02282382
    Iteration 64, loss = 0.02166635
    Iteration 65, loss = 0.02201869
    Iteration 66, loss = 0.01823042
    Iteration 67, loss = 0.01955075
    Iteration 68, loss = 0.01910996
    Iteration 69, loss = 0.01791822
    Iteration 70, loss = 0.01630146
    Iteration 71, loss = 0.01639868
    Iteration 72, loss = 0.01599290
    Iteration 73, loss = 0.01426070
    Iteration 74, loss = 0.01341884
    Iteration 75, loss = 0.01267941
    Iteration 76, loss = 0.01200319
    Iteration 77, loss = 0.01146218
    Iteration 78, loss = 0.01129495
    Iteration 79, loss = 0.01304624
    Iteration 80, loss = 0.01250818
    Iteration 81, loss = 0.01128880
    Iteration 82, loss = 0.01054352
    Iteration 83, loss = 0.01019227
    Iteration 84, loss = 0.01102079
    Iteration 85, loss = 0.00995865
    Iteration 86, loss = 0.00936145
    Iteration 87, loss = 0.00909913
    Iteration 88, loss = 0.00851930
    Iteration 89, loss = 0.00816062
    Iteration 90, loss = 0.00790792
    Iteration 91, loss = 0.00813990
    Iteration 92, loss = 0.00745901
    Iteration 93, loss = 0.00750988
    Iteration 94, loss = 0.00730929
    Iteration 95, loss = 0.00719653
    Iteration 96, loss = 0.00771336
    Iteration 97, loss = 0.00797556
    Iteration 98, loss = 0.00765823
    Iteration 99, loss = 0.00693065
    Iteration 100, loss = 0.00720773
    Iteration 101, loss = 0.00689123
    Iteration 102, loss = 0.00660444
    Iteration 103, loss = 0.00594398
    Iteration 104, loss = 0.00574787
    Iteration 105, loss = 0.00574118
    Iteration 106, loss = 0.00560719
    Iteration 107, loss = 0.00501990
    Iteration 108, loss = 0.00488525
    Iteration 109, loss = 0.00465671
    Iteration 110, loss = 0.00447407
    Iteration 111, loss = 0.00539153
    Iteration 112, loss = 0.00468263
    Iteration 113, loss = 0.00522889
    Iteration 114, loss = 0.00460719
    Iteration 115, loss = 0.00443370
    Iteration 116, loss = 0.00438757
    Iteration 117, loss = 0.00405115
    Iteration 118, loss = 0.00406574
    Iteration 119, loss = 0.00377107
    Iteration 120, loss = 0.00372779
    Iteration 121, loss = 0.00356136
    Iteration 122, loss = 0.00359109
    Iteration 123, loss = 0.00352840
    Iteration 124, loss = 0.00324452
    Iteration 125, loss = 0.00326982
    Iteration 126, loss = 0.00321866
    Iteration 127, loss = 0.00311955
    Iteration 128, loss = 0.00321996
    Iteration 129, loss = 0.00352399
    Iteration 130, loss = 0.00303685
    Iteration 131, loss = 0.00261172
    Iteration 132, loss = 0.00268140
    Iteration 133, loss = 0.00282691
    Iteration 134, loss = 0.00258236
    Iteration 135, loss = 0.00254212
    Iteration 136, loss = 0.00278997
    Iteration 137, loss = 0.00273079
    Iteration 138, loss = 0.00271475
    Iteration 139, loss = 0.00256310
    Iteration 140, loss = 0.00242705
    Iteration 141, loss = 0.00216409
    Iteration 142, loss = 0.00214530
    Iteration 143, loss = 0.00227592
    Iteration 144, loss = 0.00257598
    Iteration 145, loss = 0.00225138
    Iteration 146, loss = 0.00217247
    Iteration 147, loss = 0.00234527
    Iteration 148, loss = 0.00207391
    Iteration 149, loss = 0.00195256
    Iteration 150, loss = 0.00189469
    Iteration 151, loss = 0.00188713
    Iteration 152, loss = 0.00170921
    Iteration 153, loss = 0.00167316
    Iteration 154, loss = 0.00190851
    Iteration 155, loss = 0.00190875
    Iteration 156, loss = 0.00182730
    Iteration 157, loss = 0.00168989
    Iteration 158, loss = 0.00166010
    Iteration 159, loss = 0.00159107
    Iteration 160, loss = 0.00161426
    Iteration 161, loss = 0.00158644
    Iteration 162, loss = 0.00148080
    Iteration 163, loss = 0.00164458
    Iteration 164, loss = 0.00158100
    Iteration 165, loss = 0.00170160
    Iteration 166, loss = 0.00145966
    Iteration 167, loss = 0.00135724
    Iteration 168, loss = 0.00132112
    Iteration 169, loss = 0.00121158
    Iteration 170, loss = 0.00120500
    Iteration 171, loss = 0.00130475
    Iteration 172, loss = 0.00121825
    Iteration 173, loss = 0.00127731
    Iteration 174, loss = 0.00140711
    Iteration 175, loss = 0.00122125
    Iteration 176, loss = 0.00137000
    Iteration 177, loss = 0.00116210
    Iteration 178, loss = 0.00102193
    Iteration 179, loss = 0.00128639
    Iteration 180, loss = 0.00116877
    Iteration 181, loss = 0.00097254
    Iteration 182, loss = 0.00089670
    Iteration 183, loss = 0.00087020
    Iteration 184, loss = 0.00093336
    Iteration 185, loss = 0.00093816
    Iteration 186, loss = 0.00089786
    Iteration 187, loss = 0.00101867
    Iteration 188, loss = 0.00104801
    Iteration 189, loss = 0.00086505
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 6.69e+00 seconds
    Evaluate trained model with onnx: 1.27e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 99-100

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 100-111

.. code-block:: Python

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_onnx, errorbar=None)
    plt.title(f'Light Curves for {obj.file_name}, {num_train} Training Points')
    plt.xlabel('Time [s]')
    plt.ylabel('Apparent Magnitude')
    plt.legend(['True', 'Model'])
    plt.grid()
    plt.show()



.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002_2_00x.png 2.00x
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 11.836 seconds)


.. _sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: neural_network_brightness.ipynb <neural_network_brightness.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: neural_network_brightness.py <neural_network_brightness.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
