
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/01-light_curves/neural_network_brightness.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_01-light_curves_neural_network_brightness.py:


Neural Network Brightness
=========================

Trains a neural network to predict the brightness of a specular cube in an arbitrary lighting and observation conditions and compares the results to the truth

.. GENERATED FROM PYTHON SOURCE LINES 7-15

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    import seaborn as sns

    import mirage as mr
    import mirage.sim as mrsim








.. GENERATED FROM PYTHON SOURCE LINES 16-17

Let's define the object and the BRDF

.. GENERATED FROM PYTHON SOURCE LINES 17-19

.. code-block:: Python

    obj = mr.SpaceObject('cube.obj')
    brdf = mr.Brdf('phong', cd=0.5, cs=0.5, n=10)







.. GENERATED FROM PYTHON SOURCE LINES 20-21

We now define the Multi-Layer Perceptron (MLP) brightness model. Note that the ``layers=(150, 50, 150)`` keyword argument defines the number of neurons in each densely-connected layer.

.. GENERATED FROM PYTHON SOURCE LINES 21-22

.. code-block:: Python

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=False)







.. GENERATED FROM PYTHON SOURCE LINES 23-24

Now we train the model on a set number of training lighting and observation configurations. Usually ``1e5``-``1e6`` are required for a *good* fit

.. GENERATED FROM PYTHON SOURCE LINES 24-27

.. code-block:: Python

    num_train = int(1e3)
    mlp_bm.train(num_train)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 1.79e-03 seconds
    Iteration 1, loss = 0.22863513
    Iteration 2, loss = 0.17675297
    Iteration 3, loss = 0.15571087
    Iteration 4, loss = 0.14970790
    Iteration 5, loss = 0.14575850
    Iteration 6, loss = 0.14401973
    Iteration 7, loss = 0.14294645
    Iteration 8, loss = 0.14147612
    Iteration 9, loss = 0.14091498
    Iteration 10, loss = 0.14055945
    Iteration 11, loss = 0.13875288
    Iteration 12, loss = 0.13767547
    Iteration 13, loss = 0.13685288
    Iteration 14, loss = 0.13570884
    Iteration 15, loss = 0.13477740
    Iteration 16, loss = 0.13366131
    Iteration 17, loss = 0.13273544
    Iteration 18, loss = 0.13114127
    Iteration 19, loss = 0.12942330
    Iteration 20, loss = 0.12770661
    Iteration 21, loss = 0.12547352
    Iteration 22, loss = 0.12342313
    Iteration 23, loss = 0.12191118
    Iteration 24, loss = 0.12027125
    Iteration 25, loss = 0.11668641
    Iteration 26, loss = 0.11315234
    Iteration 27, loss = 0.11160793
    Iteration 28, loss = 0.10723697
    Iteration 29, loss = 0.10442243
    Iteration 30, loss = 0.10390597
    Iteration 31, loss = 0.09688759
    Iteration 32, loss = 0.09247636
    Iteration 33, loss = 0.08723784
    Iteration 34, loss = 0.08347957
    Iteration 35, loss = 0.07624970
    Iteration 36, loss = 0.07279532
    Iteration 37, loss = 0.07294126
    Iteration 38, loss = 0.06707134
    Iteration 39, loss = 0.06209458
    Iteration 40, loss = 0.06085298
    Iteration 41, loss = 0.05852804
    Iteration 42, loss = 0.05402143
    Iteration 43, loss = 0.05311637
    Iteration 44, loss = 0.05124528
    Iteration 45, loss = 0.05105751
    Iteration 46, loss = 0.05189811
    Iteration 47, loss = 0.05067292
    Iteration 48, loss = 0.04917045
    Iteration 49, loss = 0.04650010
    Iteration 50, loss = 0.04339354
    Iteration 51, loss = 0.04059302
    Iteration 52, loss = 0.03887314
    Iteration 53, loss = 0.03789498
    Iteration 54, loss = 0.03905422
    Iteration 55, loss = 0.03529984
    Iteration 56, loss = 0.03223724
    Iteration 57, loss = 0.03176166
    Iteration 58, loss = 0.03182013
    Iteration 59, loss = 0.03102290
    Iteration 60, loss = 0.03409653
    Iteration 61, loss = 0.03030638
    Iteration 62, loss = 0.02813514
    Iteration 63, loss = 0.02515390
    Iteration 64, loss = 0.03009474
    Iteration 65, loss = 0.03148359
    Iteration 66, loss = 0.02453695
    Iteration 67, loss = 0.02353939
    Iteration 68, loss = 0.02213003
    Iteration 69, loss = 0.02175989
    Iteration 70, loss = 0.02103714
    Iteration 71, loss = 0.02021949
    Iteration 72, loss = 0.02159446
    Iteration 73, loss = 0.02021116
    Iteration 74, loss = 0.02087495
    Iteration 75, loss = 0.01853997
    Iteration 76, loss = 0.01757020
    Iteration 77, loss = 0.01642861
    Iteration 78, loss = 0.01626783
    Iteration 79, loss = 0.01590794
    Iteration 80, loss = 0.01533822
    Iteration 81, loss = 0.01416733
    Iteration 82, loss = 0.01346040
    Iteration 83, loss = 0.01259495
    Iteration 84, loss = 0.01249252
    Iteration 85, loss = 0.01242856
    Iteration 86, loss = 0.01178477
    Iteration 87, loss = 0.01162007
    Iteration 88, loss = 0.01175728
    Iteration 89, loss = 0.01183338
    Iteration 90, loss = 0.01114491
    Iteration 91, loss = 0.01037246
    Iteration 92, loss = 0.01025941
    Iteration 93, loss = 0.00956333
    Iteration 94, loss = 0.00982981
    Iteration 95, loss = 0.00992565
    Iteration 96, loss = 0.00938587
    Iteration 97, loss = 0.00982141
    Iteration 98, loss = 0.00884522
    Iteration 99, loss = 0.00937476
    Iteration 100, loss = 0.00791139
    Iteration 101, loss = 0.00755420
    Iteration 102, loss = 0.00821025
    Iteration 103, loss = 0.00730473
    Iteration 104, loss = 0.00718073
    Iteration 105, loss = 0.00700614
    Iteration 106, loss = 0.00825340
    Iteration 107, loss = 0.00788207
    Iteration 108, loss = 0.00781191
    Iteration 109, loss = 0.00924811
    Iteration 110, loss = 0.00835852
    Iteration 111, loss = 0.00759939
    Iteration 112, loss = 0.00668938
    Iteration 113, loss = 0.00591918
    Iteration 114, loss = 0.00567129
    Iteration 115, loss = 0.00549806
    Iteration 116, loss = 0.00569065
    Iteration 117, loss = 0.00522250
    Iteration 118, loss = 0.00524278
    Iteration 119, loss = 0.00478708
    Iteration 120, loss = 0.00499833
    Iteration 121, loss = 0.00486600
    Iteration 122, loss = 0.00502306
    Iteration 123, loss = 0.00505798
    Iteration 124, loss = 0.00455769
    Iteration 125, loss = 0.00478749
    Iteration 126, loss = 0.00442855
    Iteration 127, loss = 0.00444975
    Iteration 128, loss = 0.00424734
    Iteration 129, loss = 0.00445410
    Iteration 130, loss = 0.00458468
    Iteration 131, loss = 0.00410241
    Iteration 132, loss = 0.00393880
    Iteration 133, loss = 0.00393909
    Iteration 134, loss = 0.00388546
    Iteration 135, loss = 0.00368864
    Iteration 136, loss = 0.00379863
    Iteration 137, loss = 0.00347197
    Iteration 138, loss = 0.00380552
    Iteration 139, loss = 0.00359211
    Iteration 140, loss = 0.00367316
    Iteration 141, loss = 0.00340221
    Iteration 142, loss = 0.00351077
    Iteration 143, loss = 0.00371313
    Iteration 144, loss = 0.00364382
    Iteration 145, loss = 0.00321687
    Iteration 146, loss = 0.00309253
    Iteration 147, loss = 0.00392524
    Iteration 148, loss = 0.00424473
    Iteration 149, loss = 0.00368074
    Iteration 150, loss = 0.00306081
    Iteration 151, loss = 0.00291638
    Iteration 152, loss = 0.00272222
    Iteration 153, loss = 0.00273260
    Iteration 154, loss = 0.00256122
    Iteration 155, loss = 0.00257278
    Iteration 156, loss = 0.00233371
    Iteration 157, loss = 0.00248993
    Iteration 158, loss = 0.00244370
    Iteration 159, loss = 0.00255520
    Iteration 160, loss = 0.00242506
    Iteration 161, loss = 0.00246811
    Iteration 162, loss = 0.00224871
    Iteration 163, loss = 0.00209369
    Iteration 164, loss = 0.00247820
    Iteration 165, loss = 0.00237874
    Iteration 166, loss = 0.00217313
    Iteration 167, loss = 0.00207457
    Iteration 168, loss = 0.00216396
    Iteration 169, loss = 0.00225857
    Iteration 170, loss = 0.00211662
    Iteration 171, loss = 0.00240915
    Iteration 172, loss = 0.00208471
    Iteration 173, loss = 0.00213744
    Iteration 174, loss = 0.00236225
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 3.15e+00 seconds




.. GENERATED FROM PYTHON SOURCE LINES 28-29

We can now simulate a torque-free attitude profile to inspect the quality of the fit

.. GENERATED FROM PYTHON SOURCE LINES 29-40

.. code-block:: Python

    t_eval = np.linspace(0, 10, 1000)
    q, _ = mr.propagate_attitude_torque_free(
        np.array([0.0, 0.0, 0.0, 1.0]),
        np.array([1.0, 1.0, 1.0]),
        np.diag([1, 2, 3]),
        t_eval,
    )
    dcm = mr.quat_to_dcm(q)
    ovb = mr.stack_mat_mult_vec(dcm, np.array([[1, 0, 0]]))
    svb = mr.stack_mat_mult_vec(dcm, np.array([[0, 1, 0]]))








.. GENERATED FROM PYTHON SOURCE LINES 41-42

Evaluating the model in its two available formats - as a native ``scikit-learn`` model and as an Open Neural Network eXchange (ONNX) model

.. GENERATED FROM PYTHON SOURCE LINES 42-49

.. code-block:: Python

    mr.tic('Evaluate trained model with sklearn')
    mdl_b_sklearn = mlp_bm.eval(ovb, svb, eval_mode_pref='sklearn')
    mr.toc()
    mr.tic('Evaluate trained model with onnx')
    mdl_b_onnx = mlp_bm.eval(ovb, svb, eval_mode_pref='onnx')
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate trained model with sklearn: 1.55e-03 seconds
    Evaluate trained model with onnx: 1.13e-03 seconds




.. GENERATED FROM PYTHON SOURCE LINES 50-51

We can save both of these representations to file:

.. GENERATED FROM PYTHON SOURCE LINES 51-54

.. code-block:: Python

    mlp_bm.save_to_file(save_as_format='onnx')
    mlp_bm.save_to_file(save_as_format='sklearn')








.. GENERATED FROM PYTHON SOURCE LINES 55-56

Now we load the model from its ``.onxx`` file we just saved and evaluate the brightness

.. GENERATED FROM PYTHON SOURCE LINES 56-61

.. code-block:: Python

    mlp_bm.load_from_file(mlp_bm.onnx_file_name)
    mr.tic('Evaluate loaded model with onxx')
    mdl_onnx_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref='onnx')
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with onxx: 8.09e-04 seconds




.. GENERATED FROM PYTHON SOURCE LINES 62-63

And we do the same for the ``scikit-learn`` ``.plk`` file we saved

.. GENERATED FROM PYTHON SOURCE LINES 63-68

.. code-block:: Python

    mlp_bm.load_from_file(mlp_bm.sklearn_file_name)
    mr.tic('Evaluate loaded model with sklearn')
    mdl_sklearn_loaded = mlp_bm.eval(ovb, svb, eval_mode_pref='sklearn')
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Evaluate loaded model with sklearn: 1.39e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 69-70

We can easily confirm that all four model evaluations have produced the same prediction

.. GENERATED FROM PYTHON SOURCE LINES 70-75

.. code-block:: Python

    print(np.max(np.abs(mdl_b_sklearn - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_onnx_loaded)))
    print(np.max(np.abs(mdl_b_sklearn - mdl_sklearn_loaded)))
    print(np.max(np.abs(mdl_b_onnx - mdl_sklearn_loaded)))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    8.564862485904712e-07
    0.0
    0.0
    8.564862485904712e-07




.. GENERATED FROM PYTHON SOURCE LINES 76-77

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 77-89

.. code-block:: Python

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_sklearn, errorbar=None)
    plt.title(f'Light Curves for {obj.file_name}, {num_train} Training Points')
    plt.xlabel('Time [s]')
    plt.ylabel('Normalized brightness')
    plt.legend(['True', 'Model'])
    plt.grid()
    plt.show()




.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_001_2_00x.png 2.00x
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 90-91

We can also train on magnitude data instead of irradiance:

.. GENERATED FROM PYTHON SOURCE LINES 91-98

.. code-block:: Python

    mlp_bm = mrsim.MLPBrightnessModel(obj, brdf, use_engine=True)
    mlp_bm.train(num_train)

    mr.tic('Evaluate trained model with onnx')
    mdl_b_onnx = mlp_bm.eval(ovb, svb)
    mr.toc()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Compute training LC: 4.30e-01 seconds
    Iteration 1, loss = 0.29905913
    Iteration 2, loss = 0.14494127
    Iteration 3, loss = 0.15664452
    Iteration 4, loss = 0.13117617
    Iteration 5, loss = 0.12634293
    Iteration 6, loss = 0.12591977
    Iteration 7, loss = 0.12073647
    Iteration 8, loss = 0.12009891
    Iteration 9, loss = 0.11982630
    Iteration 10, loss = 0.11768812
    Iteration 11, loss = 0.11757527
    Iteration 12, loss = 0.11605857
    Iteration 13, loss = 0.11513477
    Iteration 14, loss = 0.11378795
    Iteration 15, loss = 0.11270602
    Iteration 16, loss = 0.11203345
    Iteration 17, loss = 0.11041755
    Iteration 18, loss = 0.10853000
    Iteration 19, loss = 0.10723594
    Iteration 20, loss = 0.10413469
    Iteration 21, loss = 0.10420223
    Iteration 22, loss = 0.10116386
    Iteration 23, loss = 0.09816342
    Iteration 24, loss = 0.09573270
    Iteration 25, loss = 0.09328099
    Iteration 26, loss = 0.09057446
    Iteration 27, loss = 0.08775869
    Iteration 28, loss = 0.08466376
    Iteration 29, loss = 0.08103654
    Iteration 30, loss = 0.07700724
    Iteration 31, loss = 0.07330968
    Iteration 32, loss = 0.06927110
    Iteration 33, loss = 0.06592915
    Iteration 34, loss = 0.06196300
    Iteration 35, loss = 0.05917752
    Iteration 36, loss = 0.05638410
    Iteration 37, loss = 0.05262837
    Iteration 38, loss = 0.05024821
    Iteration 39, loss = 0.04802875
    Iteration 40, loss = 0.04641178
    Iteration 41, loss = 0.04405190
    Iteration 42, loss = 0.04112422
    Iteration 43, loss = 0.04107678
    Iteration 44, loss = 0.03843430
    Iteration 45, loss = 0.03478729
    Iteration 46, loss = 0.03317579
    Iteration 47, loss = 0.03252326
    Iteration 48, loss = 0.03079300
    Iteration 49, loss = 0.02866580
    Iteration 50, loss = 0.02800208
    Iteration 51, loss = 0.02602720
    Iteration 52, loss = 0.02594834
    Iteration 53, loss = 0.02541626
    Iteration 54, loss = 0.02484129
    Iteration 55, loss = 0.02694257
    Iteration 56, loss = 0.02255576
    Iteration 57, loss = 0.02071868
    Iteration 58, loss = 0.01986496
    Iteration 59, loss = 0.01881515
    Iteration 60, loss = 0.01940376
    Iteration 61, loss = 0.01937973
    Iteration 62, loss = 0.01812010
    Iteration 63, loss = 0.01781040
    Iteration 64, loss = 0.01679402
    Iteration 65, loss = 0.01768787
    Iteration 66, loss = 0.01600412
    Iteration 67, loss = 0.01443198
    Iteration 68, loss = 0.01445073
    Iteration 69, loss = 0.01476481
    Iteration 70, loss = 0.01356901
    Iteration 71, loss = 0.01290901
    Iteration 72, loss = 0.01275349
    Iteration 73, loss = 0.01290271
    Iteration 74, loss = 0.01190944
    Iteration 75, loss = 0.01313146
    Iteration 76, loss = 0.01290330
    Iteration 77, loss = 0.01231006
    Iteration 78, loss = 0.01078058
    Iteration 79, loss = 0.01024938
    Iteration 80, loss = 0.00901288
    Iteration 81, loss = 0.00904648
    Iteration 82, loss = 0.00857345
    Iteration 83, loss = 0.00890067
    Iteration 84, loss = 0.00828226
    Iteration 85, loss = 0.00792782
    Iteration 86, loss = 0.00748860
    Iteration 87, loss = 0.00733766
    Iteration 88, loss = 0.00741522
    Iteration 89, loss = 0.00746203
    Iteration 90, loss = 0.00698690
    Iteration 91, loss = 0.00695080
    Iteration 92, loss = 0.00643394
    Iteration 93, loss = 0.00641683
    Iteration 94, loss = 0.00629428
    Iteration 95, loss = 0.00624336
    Iteration 96, loss = 0.00580942
    Iteration 97, loss = 0.00638502
    Iteration 98, loss = 0.00556508
    Iteration 99, loss = 0.00531401
    Iteration 100, loss = 0.00508597
    Iteration 101, loss = 0.00519677
    Iteration 102, loss = 0.00500361
    Iteration 103, loss = 0.00474875
    Iteration 104, loss = 0.00487282
    Iteration 105, loss = 0.00445866
    Iteration 106, loss = 0.00411243
    Iteration 107, loss = 0.00531339
    Iteration 108, loss = 0.00468349
    Iteration 109, loss = 0.00466848
    Iteration 110, loss = 0.00437189
    Iteration 111, loss = 0.00395270
    Iteration 112, loss = 0.00374916
    Iteration 113, loss = 0.00361738
    Iteration 114, loss = 0.00343211
    Iteration 115, loss = 0.00353641
    Iteration 116, loss = 0.00345664
    Iteration 117, loss = 0.00316829
    Iteration 118, loss = 0.00318209
    Iteration 119, loss = 0.00316215
    Iteration 120, loss = 0.00294785
    Iteration 121, loss = 0.00297547
    Iteration 122, loss = 0.00295249
    Iteration 123, loss = 0.00278854
    Iteration 124, loss = 0.00268866
    Iteration 125, loss = 0.00274782
    Iteration 126, loss = 0.00274607
    Iteration 127, loss = 0.00279785
    Iteration 128, loss = 0.00274324
    Iteration 129, loss = 0.00252513
    Iteration 130, loss = 0.00251593
    Iteration 131, loss = 0.00269305
    Iteration 132, loss = 0.00268887
    Iteration 133, loss = 0.00235643
    Iteration 134, loss = 0.00233565
    Iteration 135, loss = 0.00237026
    Iteration 136, loss = 0.00256978
    Iteration 137, loss = 0.00244157
    Iteration 138, loss = 0.00238433
    Iteration 139, loss = 0.00204206
    Iteration 140, loss = 0.00202793
    Iteration 141, loss = 0.00198388
    Iteration 142, loss = 0.00178192
    Iteration 143, loss = 0.00175065
    Iteration 144, loss = 0.00176247
    Iteration 145, loss = 0.00189964
    Iteration 146, loss = 0.00172608
    Iteration 147, loss = 0.00178344
    Iteration 148, loss = 0.00175112
    Iteration 149, loss = 0.00173464
    Iteration 150, loss = 0.00163097
    Iteration 151, loss = 0.00183660
    Iteration 152, loss = 0.00194456
    Iteration 153, loss = 0.00172624
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    Fit against 1000 pts: : 8.18e+00 seconds
    Evaluate trained model with onnx: 1.62e-02 seconds




.. GENERATED FROM PYTHON SOURCE LINES 99-100

We can now finish off by evaluating the true brightness in this attitude profile and plot the results

.. GENERATED FROM PYTHON SOURCE LINES 100-111

.. code-block:: Python

    true_b = mlp_bm.brightness(svb, ovb)

    plt.figure()
    sns.lineplot(x=t_eval, y=true_b, errorbar=None)
    sns.lineplot(x=t_eval, y=mdl_b_onnx, errorbar=None)
    plt.title(f'Light Curves for {obj.file_name}, {num_train} Training Points')
    plt.xlabel('Time [s]')
    plt.ylabel('Apparent Magnitude')
    plt.legend(['True', 'Model'])
    plt.grid()
    plt.show()



.. image-sg:: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png
   :alt: Light Curves for cube.obj, 1000 Training Points
   :srcset: /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002.png, /gallery/01-light_curves/images/sphx_glr_neural_network_brightness_002_2_00x.png 2.00x
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 12.620 seconds)


.. _sphx_glr_download_gallery_01-light_curves_neural_network_brightness.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: neural_network_brightness.ipynb <neural_network_brightness.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: neural_network_brightness.py <neural_network_brightness.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: neural_network_brightness.zip <neural_network_brightness.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
